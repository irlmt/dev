{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56b0561",
   "metadata": {},
   "source": [
    "Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1847b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict, deque, Counter, OrderedDict\n",
    "from typing import List\n",
    "import json, pickle\n",
    "\n",
    "Book = namedtuple(\"Book\", [\"title\", \"author\", \"isbn\", \"year\"])\n",
    "Reader = namedtuple(\"Reader\", [\"name\", \"reader_id\", \"phone\"])\n",
    "\n",
    "list_of_books: List[str]= [\n",
    "    Book(title=\"abc\", author=\"111\", isbn=1, year=2025),\n",
    "    Book(title=\"bcd\", author=\"222\", isbn=2, year=2021),\n",
    "    Book(title=\"cde\", author=\"111\", isbn=3, year=1234),\n",
    "    Book(title=\"abc\", author=\"333\", isbn=4, year=5555),\n",
    "]\n",
    "books_by_genre: defaultdict = defaultdict(list)\n",
    "books_by_genre[\"genre1\"].append(list_of_books[0])\n",
    "books_by_genre[\"genre2\"].append(list_of_books[1])\n",
    "books_by_genre[\"genre3\"].append(list_of_books[2])\n",
    "books_by_genre[\"genre2\"].append(list_of_books[3])\n",
    "deque_of_readers: deque = deque()\n",
    "deque_of_readers.append(Reader(name=\"r1\", reader_id=1, phone=789))\n",
    "deque_of_readers.append(Reader(name=\"r2\", reader_id=11, phone=000))\n",
    "deque_of_readers.append(Reader(name=\"r5\", reader_id=5, phone=23))\n",
    "my_counter: Counter = Counter([i.author for i in list_of_books])\n",
    "my_ordered_dict: OrderedDict = OrderedDict.fromkeys(deque_of_readers)\n",
    "my_ordered_dict[Reader(name=\"r1\", reader_id=1, phone=789)] = list_of_books[0]\n",
    "my_ordered_dict[Reader(name=\"r5\", reader_id=5, phone=23)] = list_of_books[3]\n",
    "\n",
    "\n",
    "with open(\"task_1.json\", \"w\") as file:\n",
    "    json.dump(list_of_books, file)\n",
    "    json.dump(deque_of_readers, file)\n",
    "    json.dump(my_counter, file)\n",
    "    json.dump(my_ordered_dict, file)\n",
    "with open(\"task_1.pickle\", \"wb\") as file:\n",
    "    pickle.dump(\"list of books: \" + str(list_of_books), file)\n",
    "    pickle.dump(\"deque of reader: \" + str(deque_of_readers), file)\n",
    "    pickle.dump(\"counter of authors: \" + str(my_counter), file)\n",
    "    pickle.dump(\"ordered dict: \" + str(my_ordered_dict), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf3b235",
   "metadata": {},
   "source": [
    "Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceed60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict, deque, Counter\n",
    "import json, os, sys\n",
    "from datetime import datetime\n",
    "\n",
    "FileInfo: namedtuple = namedtuple(\"FileInfo\", [\"name\", \"size\", \"extension\", \"modified_time\"])\n",
    "\n",
    "files_by_size: defaultdict = defaultdict(list)\n",
    "files_list: List = []\n",
    "last_ten_files: deque = deque(maxlen=10)\n",
    "\n",
    "counter_files_extentions: Counter = Counter()\n",
    "\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        try:\n",
    "            size = sys.getsizeof(file_path)\n",
    "            mtime = os.path.getmtime(file_path)\n",
    "            modified_time = datetime.fromtimestamp(mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            name, extension = os.path.splitext(file)\n",
    "            if not extension:\n",
    "                extension = \"no_extension\"\n",
    "\n",
    "            file_info: namedtuple = FileInfo(\n",
    "                name=file, size=size, extension=extension, modified_time=modified_time\n",
    "            )\n",
    "            files_list.append(file_info)\n",
    "\n",
    "            counter_files_extentions[extension] += 1\n",
    "\n",
    "            if size < 1024 * 1024:\n",
    "                files_by_size[\"small\"].append(file_info)\n",
    "            elif size <= 100 * 1024 * 1024:\n",
    "                files_by_size[\"medium\"].append(file_info)\n",
    "            else:\n",
    "                files_by_size[\"large\"].append(file_info)\n",
    "\n",
    "            last_ten_files.append(file_info)\n",
    "\n",
    "        except (OSError, PermissionError):\n",
    "            continue\n",
    "with open(\"task_2.json\", \"w\") as file:\n",
    "    json.dump(\"list of files: \" + str(files_list), file)\n",
    "    json.dump(\"count ext: \" + str(counter_files_extentions), file)\n",
    "    json.dump(\"size groups: \" + str(files_by_size), file)\n",
    "    json.dump(\"last_10_files: \" + str(last_ten_files), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e2a4ba",
   "metadata": {},
   "source": [
    "Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb4ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict, ChainMap, OrderedDict\n",
    "import os, json, pickle\n",
    "\n",
    "Config: namedtuple = namedtuple(\"Config\", [\"key\", \"value\", \"section\", \"default_value\"])\n",
    "default: defaultdict = defaultdict(list)\n",
    "default[\"database.host\"].append(\"localhost\")\n",
    "default[\"database.port\"].append(5432)\n",
    "default[\"app.debug\"].append(False)\n",
    "default[\"app.timeout\"].append(30)\n",
    "default[\"logging.level\"].append(\"INFO\")\n",
    "\n",
    "user: defaultdict = defaultdict(list)\n",
    "user[\"database.host\"].append(\"127.0.0.1\")\n",
    "user[\"app.debug\"].append(True)\n",
    "user[\"user.name\"].append(\"admin\")\n",
    "environment: defaultdict = defaultdict()\n",
    "for key, value in os.environ.items():\n",
    "    if key.startswith(\"APP_\"):\n",
    "        environment[key[4:].lower()] = value\n",
    "prioritized_chain = ChainMap(default, user, environment)\n",
    "\n",
    "\n",
    "grouped_config: defaultdict = defaultdict(dict)\n",
    "\n",
    "for full_key in prioritized_chain:\n",
    "    value = prioritized_chain[full_key]\n",
    "\n",
    "    if \".\" in full_key:\n",
    "        section, key = full_key.split(\".\", 1)\n",
    "    else:\n",
    "        section = \"general\"\n",
    "        key = full_key\n",
    "\n",
    "    grouped_config[section][key] = value\n",
    "\n",
    "ordered_config: OrderedDict = OrderedDict()\n",
    "ordered_config[\"environment\"] = dict(environment)\n",
    "ordered_config[\"user\"] = dict(user)\n",
    "ordered_config[\"default\"] = dict(default)\n",
    "\n",
    "with open(\"task_3.json\", \"w\") as file:\n",
    "    json.dump(\"default: \" + str(default), file)\n",
    "    json.dump(\"user: \" + str(user), file)\n",
    "    json.dump(\"environment: \" + str(environment), file)\n",
    "    json.dump(\"proritized_chain: \" + str(prioritized_chain), file)\n",
    "    json.dump(\"grouped_config: \" + str(grouped_config), file)\n",
    "    json.dump(\"ordered_config: \" + str(ordered_config), file)\n",
    "with open(\"task_3.pickle\", \"wb\") as file:\n",
    "    pickle.dump(\"default: \" + str(default), file)\n",
    "    pickle.dump(\"user: \" + str(user), file)\n",
    "    pickle.dump(\"environment: \" + str(environment), file)\n",
    "    pickle.dump(\"proritized_chain: \" + str(prioritized_chain), file)\n",
    "    pickle.dump(\"grouped_config: \" + str(grouped_config), file)\n",
    "    pickle.dump(\"ordered_config: \" + str(ordered_config), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c92f0c7",
   "metadata": {},
   "source": [
    "Задание 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd858906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque, Counter, defaultdict\n",
    "import datetime\n",
    "from typing import NamedTuple\n",
    "import os, sys, pickle\n",
    "\n",
    "SystemInfo = namedtuple(\n",
    "    \"SystemInfo\", [\"cpu_count\", \"memory_usage\", \"process_id\", \"user_name\"]\n",
    ")\n",
    "\n",
    "history: deque = deque(maxlen=20)\n",
    "function_counter = Counter()\n",
    "grouped_by_time: defaultdict = defaultdict(list)\n",
    "\n",
    "\n",
    "def collect_system_info():\n",
    "    function_counter[\"os.cpu_count\"] += 1\n",
    "    cpu_count = os.cpu_count()\n",
    "\n",
    "    function_counter[\"sys.getallocatedblocks\"] += 1\n",
    "    memory_usage = sys.getallocatedblocks()\n",
    "\n",
    "    function_counter[\"os.getpid\"] += 1\n",
    "    process_id = os.getpid()\n",
    "\n",
    "    function_counter[\"os.getlogin\"] += 1\n",
    "    user_name = os.getlogin()\n",
    "\n",
    "    return SystemInfo(cpu_count, memory_usage, process_id, user_name)\n",
    "\n",
    "\n",
    "for _ in range(25):\n",
    "    info = collect_system_info()\n",
    "    current_time = datetime.now()\n",
    "\n",
    "    history.append((current_time, info))\n",
    "\n",
    "    hour_key = current_time.replace(minute=0, second=0, microsecond=0)\n",
    "    grouped_by_time[hour_key].append(info)\n",
    "\n",
    "with open(\"task_4.pickle\", \"wb\") as file:\n",
    "    pickle.dump(\n",
    "        {\n",
    "            \"history\": history,\n",
    "            \"function_counter\": function_counter,\n",
    "            \"grouped_by_time\": dict(grouped_by_time),\n",
    "        },\n",
    "        file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6eb4dd",
   "metadata": {},
   "source": [
    "Задание 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque, Counter, defaultdict, OrderedDict, ChainMap\n",
    "from typing import NamedTuple, ChainMap\n",
    "from datetime import datetime\n",
    "import os, json, pickle\n",
    "\n",
    "\n",
    "LogEntry: namedtuple = namedtuple(\n",
    "    \"LogEntry\", [\"timestamp\", \"level\", \"message\", \"module\", \"function\"]\n",
    ")\n",
    "deque_for_logs = deque(maxlen=100)\n",
    "dict_level = defaultdict(list)\n",
    "count_log_level = Counter(dict_level)\n",
    "logs_by_time = OrderedDict()\n",
    "all_logs = ChainMap(\n",
    "    {\"recent\": list(deque_for_logs)},\n",
    "    dict(dict_level),\n",
    "    {\"counter\": dict(count_log_level)},\n",
    "    dict(logs_by_time),\n",
    ")\n",
    "\n",
    "\n",
    "def add_log(level, message, module, function):\n",
    "    timestamp = datetime.now()\n",
    "    log_entry = LogEntry(timestamp, level, message, module, function)\n",
    "\n",
    "    deque_for_logs.append(log_entry)\n",
    "    dict_level[level].append(log_entry)\n",
    "    count_log_level[level] += 1\n",
    "    logs_by_time[timestamp] = log_entry\n",
    "\n",
    "    return log_entry\n",
    "\n",
    "\n",
    "add_log(\"INFO\", \"111\", \"222\", \"333\")\n",
    "add_log(\"DEBUG\", \"22\", \"33\", \"55\")\n",
    "add_log(\"ERROR\", \"777\", \"2\", \"4\")\n",
    "add_log(\"WARNING\", \"5\", \"4\", \"2\")\n",
    "\n",
    "log_dir = \"logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "json_file = os.path.join(log_dir, \"task_5.json\")\n",
    "with open(json_file, \"w\") as f:\n",
    "    json.dump([entry._asdict() for entry in deque_for_logs], f, default=str)\n",
    "\n",
    "pickle_file = os.path.join(log_dir, \"task_5.pickle\")\n",
    "with open(pickle_file, \"wb\") as f:\n",
    "    pickle.dump(\n",
    "        {\n",
    "            \"deque\": deque_for_logs,\n",
    "            \"by_level\": dict(dict_level),\n",
    "            \"counter\": count_log_level,\n",
    "            \"by_time\": logs_by_time,\n",
    "        },\n",
    "        f,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d5c39",
   "metadata": {},
   "source": [
    "Задание 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89abffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque, Counter, defaultdict, OrderedDict, ChainMap\n",
    "from typing import NamedTuple, ChainMap\n",
    "from datetime import datetime\n",
    "import os, json, pickle, sys\n",
    "\n",
    "CacheEntry: namedtuple = namedtuple(\n",
    "    \"CacheEntry\", [\"key\", \"value\", \"timestamp\", \"access_count\"]\n",
    ")\n",
    "LRU_cache: OrderedDict = OrderedDict()\n",
    "history_keys: deque = deque()\n",
    "count_keys: Counter = Counter()\n",
    "access_default = defaultdict(int)\n",
    "MAX_CACHE_SIZE = 5\n",
    "\n",
    "\n",
    "def add_to_cache(key, value):\n",
    "    if len(LRU_cache) >= MAX_CACHE_SIZE:\n",
    "        oldest_key, oldest_value = LRU_cache.popitem(last=False)\n",
    "    new_entry = CacheEntry(\n",
    "        key=key, value=value, timestamp=datetime.now(), access_count=0\n",
    "    )\n",
    "\n",
    "    LRU_cache[key] = new_entry\n",
    "\n",
    "\n",
    "def get_from_cache(key):\n",
    "    if key in LRU_cache:\n",
    "        entry = LRU_cache[key]\n",
    "        count_keys[key] += 1\n",
    "        access_default[key] += 1\n",
    "        history_keys.append((key, datetime.now()))\n",
    "        new_entry = CacheEntry(\n",
    "            key=key,\n",
    "            value=entry.value,\n",
    "            timestamp=entry.timestamp,\n",
    "            access_count=entry.access_count + 1,\n",
    "        )\n",
    "        LRU_cache.move_to_end(key)\n",
    "        LRU_cache[key] = new_entry\n",
    "\n",
    "        return new_entry.value\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def remove_from_cache(key):\n",
    "    if key in LRU_cache:\n",
    "        del LRU_cache[key]\n",
    "\n",
    "\n",
    "def clear_cache():\n",
    "    LRU_cache.clear()\n",
    "\n",
    "\n",
    "def show_cache_size():\n",
    "    size = len(LRU_cache)\n",
    "    memory = sys.getsizeof(LRU_cache)\n",
    "\n",
    "\n",
    "def save_cache(filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(dict(LRU_cache), f)\n",
    "\n",
    "\n",
    "def load_cache(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "        LRU_cache.update(loaded_data)\n",
    "\n",
    "\n",
    "add_to_cache(\"user1\", \"a\")\n",
    "add_to_cache(\"user2\", \"b\")\n",
    "add_to_cache(\"user3\", \"c\")\n",
    "add_to_cache(\"user4\", \"d\")\n",
    "add_to_cache(\"user5\", \"e\")\n",
    "\n",
    "show_cache_size()\n",
    "\n",
    "add_to_cache(\"user6\", \"f\")\n",
    "\n",
    "get_from_cache(\"user2\")\n",
    "get_from_cache(\"user1\")\n",
    "\n",
    "save_cache(\"task_6.pkl\")\n",
    "clear_cache()\n",
    "show_cache_size()\n",
    "load_cache(\"task_6.pkl\")\n",
    "show_cache_size()\n",
    "\n",
    "remove_from_cache(\"user3\")\n",
    "show_cache_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc736f25",
   "metadata": {},
   "source": [
    "Задание 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa662c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, Counter, defaultdict, deque, OrderedDict\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "WordInfo = namedtuple(\"WordInfo\", [\"word\", \"frequency\", \"length\", \"first_occurrence\"])\n",
    "\n",
    "text: str = \"aa aaa ff f ff b c\"\n",
    "words: List[str] = text.split()\n",
    "\n",
    "word_freq: Counter = Counter(words)\n",
    "word_lengths: defaultdict = defaultdict(list)\n",
    "for i in words:\n",
    "    word_lengths[len(i)].append(i)\n",
    "\n",
    "unique_words: deque = deque(maxlen=50)\n",
    "for i in words:\n",
    "    if i not in unique_words:\n",
    "        unique_words.append(i)\n",
    "\n",
    "word_order: OrderedDict = OrderedDict()\n",
    "for i in words:\n",
    "    if i not in word_order:\n",
    "        word_order[i] = datetime.now()\n",
    "\n",
    "word_stats = []\n",
    "for word, freq in word_freq.items():\n",
    "    word_stats.append(\n",
    "        WordInfo(\n",
    "            word=word,\n",
    "            frequency=freq,\n",
    "            length=len(word),\n",
    "            first_occurrence=word_order[word],\n",
    "        )\n",
    "    )\n",
    "\n",
    "memory_usage = sys.getsizeof(word_stats)\n",
    "with open(\"task_7.json\", \"w\") as f:\n",
    "    json.dump([w._asdict() for w in word_stats], f, default=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e059650",
   "metadata": {},
   "source": [
    "Задание 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a63c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict, deque, Counter, OrderedDict, ChainMap\n",
    "from typing import NamedTuple\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "Task: namedtuple = namedtuple(\n",
    "    \"Task\", [\"id\", \"title\", \"description\", \"priority\", \"status\", \"created_date\"]\n",
    ")\n",
    "tasks_by_status: defaultdict = defaultdict(list)\n",
    "priority_queue: deque = deque()\n",
    "task_counter: Counter = Counter()\n",
    "tasks_ordered: OrderedDict = OrderedDict()\n",
    "all_tasks: ChainMap = ChainMap(\n",
    "    tasks_by_status,\n",
    "    {\"priority_queue\": list(priority_queue)},\n",
    "    dict(task_counter),\n",
    "    tasks_ordered,\n",
    ")\n",
    "\n",
    "\n",
    "def add_task(title, description, priority):\n",
    "    task_id = len(tasks_ordered) + 1\n",
    "    new_task = Task(\n",
    "        id=task_id,\n",
    "        title=title,\n",
    "        description=description,\n",
    "        priority=priority,\n",
    "        status=\"todo\",\n",
    "        created_date=datetime.now(),\n",
    "    )\n",
    "    tasks_by_status[\"todo\"].append(new_task)\n",
    "    if priority == \"high\":\n",
    "        priority_queue.append(new_task)\n",
    "    task_counter[priority] += 1\n",
    "    tasks_ordered[task_id] = new_task\n",
    "\n",
    "\n",
    "def complete_task(task_id):\n",
    "    if task_id in tasks_ordered:\n",
    "        task = tasks_ordered[task_id]\n",
    "        tasks_by_status[task.status].remove(task)\n",
    "        tasks_by_status[\"done\"].append(task._replace(status=\"done\"))\n",
    "        if task in priority_queue:\n",
    "            priority_queue.remove(task)\n",
    "\n",
    "\n",
    "def get_tasks_by_status(status):\n",
    "    return tasks_by_status.get(status, [])\n",
    "\n",
    "\n",
    "def get_priority_queue():\n",
    "    return list(priority_queue)\n",
    "\n",
    "\n",
    "with open(\"task_8.json\", \"w\") as f:\n",
    "    json.dump([t._asdict() for t in tasks_ordered.values()], f, default=str)\n",
    "with open(\"task_8.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dict(tasks_ordered), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee7e592",
   "metadata": {},
   "source": [
    "Задание 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b596d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque, defaultdict, Counter, OrderedDict\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "PerformanceMetric: namedtuple = namedtuple(\n",
    "    \"PerformanceMetric\",\n",
    "    [\"function_name\", \"execution_time\", \"memory_usage\", \"timestamp\"],\n",
    ")\n",
    "metrics_deque: deque = deque(maxlen=100)\n",
    "metrics_by_function: defaultdict = defaultdict(list)\n",
    "function_calls: Counter = Counter()\n",
    "metrics_ordered: OrderedDict = OrderedDict()\n",
    "\n",
    "\n",
    "def record_metric(function_name, execution_time, memory_usage):\n",
    "    metric = PerformanceMetric(\n",
    "        function_name=function_name,\n",
    "        execution_time=execution_time,\n",
    "        memory_usage=memory_usage,\n",
    "        timestamp=datetime.now(),\n",
    "    )\n",
    "    metrics_deque.append(metric)\n",
    "    metrics_by_function[function_name].append(metric)\n",
    "    function_calls[function_name] += 1\n",
    "    metrics_ordered[datetime.now()] = metric\n",
    "\n",
    "\n",
    "def get_function_stats(function_name):\n",
    "    return metrics_by_function.get(function_name, [])\n",
    "\n",
    "\n",
    "def get_memory_usage():\n",
    "    return (\n",
    "        sys.getsizeof(metrics_deque)\n",
    "        + sys.getsizeof(metrics_by_function)\n",
    "        + sys.getsizeof(function_calls)\n",
    "        + sys.getsizeof(metrics_ordered)\n",
    "    )\n",
    "\n",
    "\n",
    "def export_metrics():\n",
    "    with open(\"task_9.json\", \"w\") as f:\n",
    "        json.dump([m._asdict() for m in metrics_deque], f, default=str)\n",
    "    with open(\"task_9.pkl\", \"wb\") as f:\n",
    "        pickle.dump(list(metrics_deque), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
